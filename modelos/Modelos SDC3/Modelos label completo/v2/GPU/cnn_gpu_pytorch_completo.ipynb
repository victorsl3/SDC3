{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21953404Victor/SDC3/tutorial-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import optuna\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos a partir de los archivos FITS\n",
    "path_datos = \"/home/21953404Victor/SDC3/SDC3GIT/data_actualizada/\"\n",
    "path_repo=\"/home/21953404Victor/SDC3/SDC3GIT/\"\n",
    "path = path_datos + \"ZW3.msw_image.fits\"\n",
    "hdul = fits.open(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformación del catálogo de fuentes completo (almacenado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ruta_carpeta_catalogos = \"/home/21953404Victor/SDC3/SDC3GIT/catalogos/catalogo completo 2/resultados/resultados_detection4\"\n",
    "# ruta_carpeta_resultados = \"/home/21953404Victor/SDC3/SDC3GIT/catalogos/catalogo completo 2/labels\"\n",
    "# archivos = os.listdir(ruta_carpeta_catalogos)\n",
    "\n",
    "# # Función para crear un archivo de anotación para cada catálogo\n",
    "# def crear_anotaciones(path_cat, path_anotacion):\n",
    "#     cat = Table.read(path_cat, format=\"ascii\")\n",
    "#     clase = \"fuente\"\n",
    "\n",
    "#     with open(path_anotacion, \"w\") as f:\n",
    "#         for row in cat:\n",
    "#             x = row[\"X_IMAGE\"]\n",
    "#             y = row[\"Y_IMAGE\"]\n",
    "#             f.write(f\"{x} {y} {clase}\\n\")\n",
    "\n",
    "# # Procesar todos los archivos de catálogo en la carpeta\n",
    "# for archivo in archivos:\n",
    "#     # Extraer el número de kHz del nombre del archivo\n",
    "#     num_khz = re.search(r\"(\\d+)kHz\", archivo).group(1)\n",
    "\n",
    "#     # Crear un nombre para el archivo de anotación correspondiente\n",
    "#     anotacion_nombre = f\"catalogo_completo_1_{num_khz}kHz.txt\"\n",
    "\n",
    "#     # Crear el archivo de anotación para el catálogo actual\n",
    "#     crear_anotaciones(\n",
    "#         os.path.join(ruta_carpeta_catalogos, archivo),\n",
    "#         os.path.join(ruta_carpeta_resultados, anotacion_nombre)\n",
    "#    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_carpeta_resultados = \"/home/21953404Victor/SDC3/SDC3GIT/catalogos/catalogo completo 1/labels\"\n",
    "# archivos = os.listdir(ruta_carpeta_resultados)\n",
    "\n",
    "# def clamp(value, min_value, max_value):\n",
    "#     return max(min(value, max_value), min_value)\n",
    "\n",
    "# def procesar_archivo(path_txt):\n",
    "#     with open(path_txt, \"r\") as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     label = np.zeros((2048, 2048), dtype=int)\n",
    "\n",
    "#     for line in lines:\n",
    "#         x, y, _ = line.split()\n",
    "#         x, y = int(float(x)), int(float(y))\n",
    "\n",
    "#         for i in range(-4, 4):\n",
    "#             for j in range(-4, 4):\n",
    "#                 x_coord = clamp(x + i, 0, 2047)\n",
    "#                 y_coord = clamp(y + j, 0, 2047)\n",
    "#                 label[y_coord, x_coord] = 1\n",
    "\n",
    "#     return label\n",
    "\n",
    "# # Crear una lista para guardar los datos procesados\n",
    "# target=[]\n",
    "\n",
    "# # Procesar todos los archivos de texto en la carpeta de resultados\n",
    "# for archivo in archivos:\n",
    "#     path_txt = os.path.join(ruta_carpeta_resultados, archivo)\n",
    "#     label = procesar_archivo(path_txt)\n",
    "#     target.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Configurar el dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos y dividirlos en conjuntos de entrenamiento y validación\n",
    "num_samples = len(hdul[0].data) # Cargar todos los datos en lugar de una muestra de todas las imágenes\n",
    "all_data = hdul[0].data\n",
    "indices = sorted(np.random.choice(all_data.shape[0], num_samples, replace=False))\n",
    "data = all_data[indices]\n",
    "\n",
    "input_data = np.array(data).astype(\"float32\") / 255.0 # Normalizar los datos de entrada (en formato de 8 bits implica que cad píxel tenga un valor de 0 a 255)\n",
    "input_data = input_data.reshape((data.shape[0], 1, data.shape[1], data.shape[2]))\n",
    "\n",
    "del all_data\n",
    "del hdul\n",
    "del data\n",
    "\n",
    "# En lugar de repetir el label ahora se carga la lista de labels. El resto del proceso es igual\n",
    "output_data = np.array(fits.open(path_repo+'catalogos/catalogo completo 2/fit/LABEL_COMPLETO_V2.fit')[0].data).astype(\"float32\")#[:, np.newaxis, :, :] # Agregar una dimensión para mantener el formato de los datos de entrada de la red neuronal\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(input_data, output_data, test_size=0.2, random_state=42)\n",
    "\n",
    "del input_data\n",
    "del output_data\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train).unsqueeze(1)\n",
    "x_val = torch.from_numpy(x_val)\n",
    "y_val = torch.from_numpy(y_val).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(dropout_rate, out_channels1, out_channels2, out_channels3):\n",
    "    decoder = nn.Sequential(\n",
    "        nn.ConvTranspose2d(out_channels3, out_channels2, 3, stride=2, padding=1, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(out_channels2),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.ConvTranspose2d(out_channels2, out_channels1, 3, stride=2, padding=1, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(out_channels1),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.ConvTranspose2d(out_channels1, 1, 3, padding=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    return decoder.to(device)\n",
    "\n",
    "def create_encoder(input_channels, num_filters1, num_filters2, dropout_rate, out_channels3):\n",
    "    encoder = nn.Sequential(\n",
    "        nn.Conv2d(input_channels, num_filters1, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(num_filters1),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Conv2d(num_filters1, num_filters2, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(num_filters2),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Conv2d(num_filters2, out_channels3, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout_rate)\n",
    "    )\n",
    "    return encoder.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizador de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Cargar y dividir tus datos en conjuntos de entrenamiento y validación\n",
    "    train_data = TensorDataset(x_train, y_train)\n",
    "    val_data = TensorDataset(x_val, y_val)    \n",
    "    \n",
    "    # Crear DataLoaders para entrenamiento y validación\n",
    "    tam_lote = 1\n",
    "    train_loader = DataLoader(train_data, batch_size=tam_lote, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=tam_lote, shuffle=False)\n",
    "\n",
    "    # Hiperparámetros sugeridos por Optuna\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
    "    dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5)\n",
    "    \n",
    "    # Aquí también sugerimos el número de canales en las capas de convolución del decoder\n",
    "    out_channels1 = trial.suggest_int(\"out_channels1\", 16, 64)\n",
    "    out_channels2 = trial.suggest_int(\"out_channels2\", 16, 64)\n",
    "    out_channels3 = trial.suggest_int(\"out_channels3\", 16, 64)\n",
    "    \n",
    "    # Sugerir el número de filtros en las capas de convolución del encoder\n",
    "    num_filters1 = trial.suggest_int(\"num_filters1\", 16, 64)\n",
    "\n",
    "    num_filters2 = trial.suggest_int(\"num_filters2\", 16, 64)\n",
    "\n",
    "    # Imprimir los hiperparámetros utilizados en este trial\n",
    "    print(f\"Hiperparámetros del trial {trial.number}:\")\n",
    "    print(f\"lr: {lr}, dropout_rate: {dropout_rate}\")\n",
    "    print(f\"out_channels1: {out_channels1}, out_channels2: {out_channels2}, out_channels3: {out_channels3}\")\n",
    "    print(f\"num_filters1: {num_filters1}, num_filters2: {num_filters2}\")\n",
    "\n",
    "    # Crear el modelo de 'encoder' y 'decoder' con los hiperparámetros sugeridos\n",
    "    input_channels = 1\n",
    "    encoder = create_encoder(input_channels, num_filters1, num_filters2, dropout_rate, out_channels3)\n",
    "    decoder = create_decoder(dropout_rate, out_channels1, out_channels2, out_channels3)\n",
    "\n",
    "    # Función de pérdida y optimizador\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "\n",
    "    # Crear un directorio para guardar los modelos intermedios, si aún no existe\n",
    "    os.makedirs(\"modelos_intermedios\", exist_ok=True)\n",
    "\n",
    "    # Entrenamiento y validación del modelo\n",
    "    num_epochs = 50\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            #train\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                enc_output = encoder(images)\n",
    "                dec_output = decoder(enc_output)\n",
    "\n",
    "                # Convertir las etiquetas a float\n",
    "                labels_float = labels.float()\n",
    "\n",
    "                loss = criterion(dec_output, labels_float)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "            train_losses.append(train_loss / len(train_loader))\n",
    "            \n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            #test\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    enc_output = encoder(images)\n",
    "                    dec_output = decoder(enc_output)\n",
    "                    dec_output = torch.squeeze(dec_output, dim=1) \n",
    "                    \n",
    "                    # Convertir las etiquetas a float\n",
    "                    labels_float = labels.float()\n",
    "                    labels_float = torch.squeeze(labels_float, dim=1)\n",
    "                    \n",
    "                    loss = criterion(dec_output, labels_float)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            val_losses.append(val_loss / len(val_loader))\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Pérdida del train: {train_losses[-1]}, Pérdida del test: {val_losses[-1]}\")\n",
    "\n",
    "        # Guardar los modelos intermedios\n",
    "        trial_number = trial.number\n",
    "        decoder_path = f\"modelos_intermedios/decoder_gpu_cat_completo_trial_{trial_number}.pt\"\n",
    "        encoder_path = f\"modelos_intermedios/encoder_gpu_cat_completo_trial_{trial_number}.pt\"\n",
    "        \n",
    "        torch.save(decoder.state_dict(), decoder_path)\n",
    "        torch.save(encoder.state_dict(), encoder_path)\n",
    "\n",
    "        # Limpiar la GPU\n",
    "        del decoder\n",
    "        del encoder\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Guardar el plot de pérdidas por epoch\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"modelos_intermedios/loss_plot_trial_{trial_number}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        return val_losses[-1]  # Devolver la pérdida de validación del último epoch\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(f\"[W] El trial {trial.number} falló debido a un error de falta de memoria en la GPU\")\n",
    "            print(\"Continuando con la optimización de parámetros, ignorando esta última configuración\")\n",
    "            \n",
    "            # Limpiar la GPU\n",
    "            del decoder\n",
    "            del encoder\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Asignar un valor alto para que este trial no sea seleccionado como el mejor\n",
    "            return float(\"inf\")\n",
    "        else:\n",
    "            # Vuelve a generar la excepción si el error no es \"CUDA out of memory\"\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-22 21:56:23,818]\u001b[0m A new study created in memory with name: no-name-da4f3137-0232-4807-8efb-f323913dea1b\u001b[0m\n",
      "/tmp/ipykernel_59891/1417001533.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
      "/tmp/ipykernel_59891/1417001533.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparámetros del trial 0:\n",
      "lr: 0.0009758457565748282, dropout_rate: 0.34667579337970894\n",
      "out_channels1: 43, out_channels2: 26, out_channels3: 51\n",
      "num_filters1: 40, num_filters2: 56\n",
      "Epoch 1/50, Pérdida del train: 0.10779799414902097, Pérdida del test: 97.72823557142395\n",
      "Epoch 2/50, Pérdida del train: 0.04167606332080646, Pérdida del test: 97.42884472588808\n",
      "Epoch 3/50, Pérdida del train: 0.039118013049786286, Pérdida del test: 1.6033684351167625\n",
      "Epoch 4/50, Pérdida del train: 0.0375366442060719, Pérdida del test: 0.7136143660018457\n"
     ]
    }
   ],
   "source": [
    "# Crear un estudio de optimización\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Ejecutar el estudio de optimización con la función 'objective' y un número determinado de ensayos\n",
    "study.optimize(objective, n_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('study.pkl', 'wb') as f:\n",
    "    pickle.dump(study, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor prueba: \n",
      "FrozenTrial(number=2, state=TrialState.COMPLETE, values=[0.16273348724973793], datetime_start=datetime.datetime(2023, 5, 23, 0, 42, 24, 873186), datetime_complete=datetime.datetime(2023, 5, 23, 1, 50, 31, 341843), params={'lr': 0.00010027415861131253, 'dropout_rate': 0.09585204102585937, 'out_channels1': 44, 'out_channels2': 28, 'out_channels3': 26, 'num_filters1': 61, 'num_filters2': 17}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'lr': FloatDistribution(high=0.001, log=True, low=1e-05, step=None), 'dropout_rate': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'out_channels1': IntDistribution(high=64, log=False, low=16, step=1), 'out_channels2': IntDistribution(high=64, log=False, low=16, step=1), 'out_channels3': IntDistribution(high=64, log=False, low=16, step=1), 'num_filters1': IntDistribution(high=64, log=False, low=16, step=1), 'num_filters2': IntDistribution(high=64, log=False, low=16, step=1)}, trial_id=2, value=None)\n",
      "\n",
      "Mejores hiperparámetros: \n",
      "{'lr': 0.00010027415861131253, 'dropout_rate': 0.09585204102585937, 'out_channels1': 44, 'out_channels2': 28, 'out_channels3': 26, 'num_filters1': 61, 'num_filters2': 17}\n"
     ]
    }
   ],
   "source": [
    "with open('study.pkl', 'rb') as f:\n",
    "    study = pickle.load(f)\n",
    "\n",
    "print(\"Mejor prueba: \")\n",
    "print(study.best_trial)\n",
    "\n",
    "print(\"\\nMejores hiperparámetros: \")\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados:\n",
      "Tasa de aprendizaje: 0.00010027415861131253\n",
      "Tasa de dropout: 0.09585204102585937\n",
      "Número de filtros en la primera capa convolucional del encoder: 61\n",
      "Número de filtros en la segunda capa convolucional del encoder: 17\n",
      "Número de canales en la primera capa del decoder: 44\n",
      "Número de canales en la segunda capa del decoder: 28\n",
      "Número de canales en la tercera capa del decoder: 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(\"Tasa de aprendizaje:\", study.best_params[\"lr\"])\n",
    "print(\"Tasa de dropout:\", study.best_params[\"dropout_rate\"])\n",
    "print(\"Número de filtros en la primera capa convolucional del encoder:\", study.best_params[\"num_filters1\"])\n",
    "print(\"Número de filtros en la segunda capa convolucional del encoder:\", study.best_params[\"num_filters2\"])\n",
    "print(\"Número de canales en la primera capa del decoder:\", study.best_params[\"out_channels1\"])\n",
    "print(\"Número de canales en la segunda capa del decoder:\", study.best_params[\"out_channels2\"])\n",
    "print(\"Número de canales en la tercera capa del decoder:\", study.best_params[\"out_channels3\"])\n",
    "\n",
    "# Obtener el número del mejor ensayo\n",
    "best_trial_number = study.best_trial.number\n",
    "\n",
    "# Crear un nuevo 'decoder' y 'encoder' con los mejores hiperparámetros encontrados\n",
    "decoder = create_decoder(\n",
    "    study.best_params[\"dropout_rate\"],\n",
    "    study.best_params[\"out_channels1\"],\n",
    "    study.best_params[\"out_channels2\"],\n",
    "    study.best_params[\"out_channels3\"])\n",
    "\n",
    "encoder = create_encoder(\n",
    "    1, # número de canales de entrada\n",
    "    study.best_params[\"num_filters1\"],\n",
    "    study.best_params[\"num_filters2\"],\n",
    "    study.best_params[\"dropout_rate\"],\n",
    "    study.best_params[\"out_channels3\"]\n",
    "    )\n",
    "\n",
    "# Cargar los estados de los mejores modelos\n",
    "decoder_path = f\"modelos_intermedios/decoder_gpu_cat_completo_trial_{best_trial_number}.pt\"\n",
    "encoder_path = f\"modelos_intermedios/encoder_gpu_cat_completo_trial_{best_trial_number}.pt\"\n",
    "\n",
    "decoder.load_state_dict(torch.load(decoder_path))\n",
    "encoder.load_state_dict(torch.load(encoder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_74009/1435658589.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i, trial in enumerate(study.trials, 1):\n",
    "    row = trial.params\n",
    "    row['loss last 20'] = round(trial.value, 4)\n",
    "    row['rank'] = i\n",
    "    row['nº ensayo'] = str(trial.number)\n",
    "    if 'channels' in row:\n",
    "        row['ch.'] = row.pop('channels')\n",
    "    if 'filters' in row:\n",
    "        row['filt.'] = row.pop('filters')\n",
    "    if 'lr' in row:\n",
    "        row['learning rate'] = round(row.pop('lr'), 4)\n",
    "    df = df.append(row, ignore_index=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype.kind in 'fc':\n",
    "        df[col] = df[col].round(4)\n",
    "\n",
    "df = df.sort_values(by='loss last 20')\n",
    "df['rank'] = range(1, len(df) + 1)\n",
    "df.set_index('rank', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Guarda el DataFrame como un archivo Excel\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df\u001b[39m.\u001b[39;49mto_excel(\u001b[39m'\u001b[39;49m\u001b[39mranking_modelos.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/SDC3/tutorial-env/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SDC3/tutorial-env/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SDC3/tutorial-env/lib/python3.10/site-packages/pandas/core/generic.py:2374\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mformats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexcel\u001b[39;00m \u001b[39mimport\u001b[39;00m ExcelFormatter\n\u001b[1;32m   2363\u001b[0m formatter \u001b[39m=\u001b[39m ExcelFormatter(\n\u001b[1;32m   2364\u001b[0m     df,\n\u001b[1;32m   2365\u001b[0m     na_rep\u001b[39m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2372\u001b[0m     inf_rep\u001b[39m=\u001b[39minf_rep,\n\u001b[1;32m   2373\u001b[0m )\n\u001b[0;32m-> 2374\u001b[0m formatter\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m   2375\u001b[0m     excel_writer,\n\u001b[1;32m   2376\u001b[0m     sheet_name\u001b[39m=\u001b[39;49msheet_name,\n\u001b[1;32m   2377\u001b[0m     startrow\u001b[39m=\u001b[39;49mstartrow,\n\u001b[1;32m   2378\u001b[0m     startcol\u001b[39m=\u001b[39;49mstartcol,\n\u001b[1;32m   2379\u001b[0m     freeze_panes\u001b[39m=\u001b[39;49mfreeze_panes,\n\u001b[1;32m   2380\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m   2381\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2382\u001b[0m )\n",
      "File \u001b[0;32m~/SDC3/tutorial-env/lib/python3.10/site-packages/pandas/io/formats/excel.py:944\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[1;32m    940\u001b[0m     need_save \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m     \u001b[39m# error: Cannot instantiate abstract class 'ExcelWriter' with abstract\u001b[39;00m\n\u001b[1;32m    943\u001b[0m     \u001b[39m# attributes 'engine', 'save', 'supported_extensions' and 'write_cells'\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m     writer \u001b[39m=\u001b[39m ExcelWriter(  \u001b[39m# type: ignore[abstract]\u001b[39;49;00m\n\u001b[1;32m    945\u001b[0m         writer, engine\u001b[39m=\u001b[39;49mengine, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     need_save \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/SDC3/tutorial-env/lib/python3.10/site-packages/pandas/io/excel/_openpyxl.py:56\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     45\u001b[0m     path: FilePath \u001b[39m|\u001b[39m WriteExcelBuffer \u001b[39m|\u001b[39m ExcelWriter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mopenpyxl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mworkbook\u001b[39;00m \u001b[39mimport\u001b[39;00m Workbook\n\u001b[1;32m     58\u001b[0m     engine_kwargs \u001b[39m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m     60\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m     61\u001b[0m         path,\n\u001b[1;32m     62\u001b[0m         mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m         engine_kwargs\u001b[39m=\u001b[39mengine_kwargs,\n\u001b[1;32m     66\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "# Guarda el DataFrame como un archivo Excel\n",
    "df.to_excel('ranking_modelos.xlsx', index=False, engine=\"openpyxl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
