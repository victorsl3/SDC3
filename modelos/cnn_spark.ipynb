{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 17:41:16.473586: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 17:41:16.585790: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-20 17:41:17.067669: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hadoop/lib/native:\n",
      "2023-04-20 17:41:17.067725: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hadoop/lib/native:\n",
      "2023-04-20 17:41:17.067730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/20 17:41:20 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Galaxy Classification\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer y procesar los archivos FITS y CAT\n",
    "def read_fits_and_cat(file_path):\n",
    "    # Aquí va el código para leer y procesar los archivos FITS y CAT\n",
    "    # utilizando Astropy y el resto del código que has proporcionado.\n",
    "    # Finalmente, devuelve una lista de tuplas con los datos de entrada y salida.\n",
    "    pass\n",
    "\n",
    "# Cambia la ruta para que apunte a un directorio con los archivos\n",
    "fits_and_cat_files = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee y procesa los archivos FITS y CAT\n",
    "data = spark.sparkContext.wholeTextFiles(fits_and_cat_files).flatMap(lambda x: read_fits_and_cat(x[0]))\n",
    "\n",
    "# Convierte los datos en un DataFrame de Spark\n",
    "schema = [\"features\", \"label\"]\n",
    "data = data.map(lambda x: Row(**dict(zip(schema, (Vectors.dense(x[0]), x[1])))))\n",
    "data = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y validación\n",
    "train, val = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Convierte los DataFrames de Spark a RDDs para usar con Elephas\n",
    "train_rdd = train.rdd.map(lambda x: (x.features.toArray(), x.label))\n",
    "val_rdd = val.rdd.map(lambda x: (x.features.toArray(), x.label))\n",
    "\n",
    "# Convierte los RDDs a arrays NumPy para usar con Keras\n",
    "x_train, y_train = zip(*train_rdd.collect())\n",
    "x_val, y_val = zip(*val_rdd.collect())\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo de Keras\n",
    "def create_keras_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(2048, 2048, 1)))\n",
    "    model.add(tfa.layers.InstanceNormalization())\n",
    "    model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "    model.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "    model.add(tfa.layers.InstanceNormalization())\n",
    "    model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "    model.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    model.add(Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Entrena el modelo con Elephas\n",
    "estimator = ElephasEstimator()\n",
    "estimator.set_keras_model_config(create_keras_model().to_yaml())\n",
    "estimator.set_optimizer_config('adam')\n",
    "estimator.set_loss('binary_crossentropy')\n",
    "estimator.set_mode('synchronous')\n",
    "estimator.set_metrics(['accuracy'])\n",
    "\n",
    "# Convierte los datos de entrenamiento y validación en RDD\n",
    "train_rdd = to_simple_rdd(spark.sparkContext, x_train, y_train)\n",
    "val_rdd = to_simple_rdd(spark.sparkContext, x_val, y_val)\n",
    "\n",
    "# Entrena el modelo\n",
    "fitted_model = estimator.fit(train_rdd)\n",
    "\n",
    "# Evalúa el modelo\n",
    "score = fitted_model.evaluate(val_rdd)\n",
    "print(\"Accuracy: \", score[1])\n",
    "\n",
    "# Guarda el modelo\n",
    "fitted_model.save(\"cnn_spark.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
